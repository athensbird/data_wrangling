{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction & Background "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In brief, this data wrangling project is meant to utilize the skills I learned through the Udacity courses to gather, extract, assess, clean, analyze, and visualize data sources of various formats and distributions. There are three main data sources for this project: @WeRateDog's Twitter archive file provided by Udacity, `image_predictions.tsv` also fetched from Udacity's server, and lastly, tweetdata fetched using Twitter's own API and Python's `tweepy` libary.\n",
    "\n",
    "In terms of the data context, the tweets are posted regularly by @WeRateDogs to rate dogs in its own standards and metrics, attached with a photo of the dog being rated. The archive file has the text, source, ratings data among others, whereas `image_predictions.tsv` contain three top predictions of the breed of the dog with its probability. The data fetched from Twitter API contains each tweet's `retweet_count` and `favorite_count`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project, I'm using the three-step process (gather, assess, and clean) as suggested by Udacity to process and analyze the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Gathering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the data from the first two data source was easy. I managed to use Panda's own `read_csv` method and Python's `request` library to fetch the data from Udacity's server without much hassle. \n",
    "\n",
    "To fetch the data with Twitter API, I first completed the developer's account registration process on Twitter's website, created my own API app, then utilized its credentials as well as the `tweepy` library to build my Twitter API object. The idea is that I first queried the tweet individually using the indexes from the archive, then stored the JSON responce row by row in the destination file. Afterwards, I'll go though the written file line by line, load the JSON data using `json.loads` function, then extract `retweet_count` and `favorite_count` from the line and put it into an array of dictionaries. Lastly, I converted the array into a Panda dataframe with the handy `pd.DataFrame` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have three Pandas dataframes - `archive`, `image_predictions` and `api_data`. I took a look at the basic shape and granularity utilizing Pandas' `info()`, `shape` and `sample()`, among others, and just visually assessed the data. If there's any thing peculiar that I noticed, such as 0 favorite count for a tweet with a high number of retweets, I'll dive deeper and programatically examine the cause of the issues that arose during the investigation process. Lastly, I compiled all my findings into a list of issues that need to be cleaned later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the tidiness and quality issues have been determined, the cleaning process was relatively straightforward. Firstly, I performed operations on the dataframe schema to merge sources, expand and drop unnecessary columns, and converted column data types. Then, I performed individual operations on columns to extract, reformat, or remove data. There's only one tricky part when I tried to figure out why a lot of rows has 0 `favorite_count` while have a high `retweet_count`, I had to query the Twitter API again individually to fetch the accurate data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Honestly, this process was not a cakewalk and I had spent more time on it than any other projects in this nanodegree. However, I managed to complete the project with the tutorials, Google, and StackOverFlow. I got to know the Pandas library a lot better and learned a lot of useful tricks for data wrangling. Mostly importantly, through this project I now have a pretty clear understanding of the data wrangling process and how to put it into practice, which is invaluable to my future journey in data analysis. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
